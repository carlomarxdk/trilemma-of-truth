{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Source Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geonamescache import GeonamesCache\n",
    "from geonamescache.mappers import country\n",
    "from collections import defaultdict\n",
    "import polars as pl\n",
    "from utils import CityCountry\n",
    "from pathlib import Path\n",
    "import inflect\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "# Parent directory\n",
    "parent_dir = str(Path().resolve().parents[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Extract <City, Country> pairs from GeoNames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14680"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "country_mapper = country(from_key='iso', to_key='name')\n",
    "engine = inflect.engine()\n",
    "gc = GeonamesCache()\n",
    "city_set = set()\n",
    "for entry in gc.get_cities().values():\n",
    "    if entry['name'] != '':\n",
    "        city_set.update([entry['name']])\n",
    "\n",
    "\n",
    "city_dict = defaultdict(set)\n",
    "city_population = defaultdict(int)\n",
    "for city_name in city_set:\n",
    "    cities = gc.get_cities_by_name(city_name)\n",
    "    for city in cities:\n",
    "\n",
    "        city = list(city.values())[0]\n",
    "        keys = city.keys()\n",
    "        if 'name' in keys and 'countrycode' in keys and 'population' in keys:\n",
    "            country_name = country_mapper(city['countrycode'])\n",
    "            admin1code = city.get('admin1code')\n",
    "            if country_name is not None and city['population'] is not None:\n",
    "                if admin1code is not None and str.isalpha(admin1code) and country_name == 'United States':\n",
    "                      country_name = f\"{country_name}, {city['admin1code']}\"\n",
    "                if city['population'] > 30_000:\n",
    "                    if any(keyword.lower() in country_name.lower() for keyword in ['Republic', 'United', 'Kingdom', 'of', 'Territory', \n",
    "                                                                   'South', 'North', 'Island', 'Islands', 'Coast', 'Central', 'Netherlands',\n",
    "                                                                   ]):\n",
    "                        country_name = 'the ' + country_name\n",
    "                    elif engine.singular_noun(country_name) != False:\n",
    "                        country_name = 'the ' + country_name\n",
    "                    city_dict[city['name']].add(country_name)\n",
    "                    if city_population[city['name']] < city['population']:\n",
    "                        city_population[city['name']] = city['population'] # max population associated with the city\n",
    "city_dict = {k: list(v) for k, v in city_dict.items()}\n",
    "len(city_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Instantiate a Data Generation object\n",
    "db = CityCountry(city_dict, category='cities')\n",
    "data = db.generate_full_dataset()\n",
    "data.write_json(f\"{parent_dir}/datasets/generators/city_country.json\") # this generates a full dataset with all cities and countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we downsample the full dataset to 5500 samples\n",
    "top_cities = sorted(city_population, key=city_population.get, reverse=True)[:700]\n",
    "other_cities = set(db.keys) - set(top_cities)\n",
    "other_cities = np.random.choice(list(other_cities), 700, replace=False)\n",
    "cities = top_cities + list(other_cities)\n",
    "subsample = db.generate_subsample(n=5500, seed=42, objects=cities).with_columns(\n",
    "                 pl.col(\"correct_object_2\").list.join(\", \").alias(\"correct_object_2\"))\n",
    "subsample.write_csv(f\"{parent_dir}/datasets/_city_country_subsample.csv\")\n",
    "## THIS PART OF CODE MIGHT OUTPUT DIFFERENT RESULT (DUE TO THE ISSUE WITH THE POLARS LIBRARY) -- THE DATASET WE USED IS PROVIDED IN THE REPOSITORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsample.group_by(['correct', 'negated']).len()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic Cities and Countries\n",
    "Here, we generate synthetic names for countries and cities. \n",
    "Generated names are stored in `datasets/generators/synthetic/*_raw.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_city_name(name):\n",
    "    results = gc.search_cities(name)\n",
    "    results_1 = gc.get_cities_by_name(name)\n",
    "\n",
    "    if len(results) == 0 and len(results_1) == 0:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def validate_country_name(name):\n",
    "    if name not in db.values:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.set_random_seed(42)\n",
    "city_names = sorted(data['object_1'].unique().to_list())\n",
    "random.seed(42)\n",
    "country_names = sorted(db.values) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from namemaker import NameSet, validate_town\n",
    "import namemaker\n",
    "\n",
    "seed = 'udaxihhexdvxrcsnbacghqtargwuwr'\n",
    "random.seed(seed)\n",
    "namemaker_rng = namemaker.get_rng()\n",
    "namemaker_rng.seed(seed)\n",
    "\n",
    "city_NS = NameSet(names = city_names)\n",
    "cities_synth = [city_NS.make_name(add_to_history=False, validation_func=validate_town) for _ in range(400)]\n",
    "cities_synth = list(set(cities_synth))\n",
    "# Validate\n",
    "cities_validated = []\n",
    "for item in cities_synth:\n",
    "    if validate_city_name(item):\n",
    "        pass\n",
    "    else:\n",
    "        cities_validated.append(item)\n",
    "with open(f\"{parent_dir}/datasets/synthetic/cities_raw.txt\", 'w') as f:\n",
    "    f.write(\"\\n\".join(map(str, cities_validated)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from namemaker import NameSet\n",
    "import namemaker\n",
    "\n",
    "seed = 'udaxihhexdvxrcsnbacghqtargwuwr'\n",
    "random.seed(seed)\n",
    "namemaker_rng = namemaker.get_rng()\n",
    "namemaker_rng.seed(seed)\n",
    "\n",
    "country_NS = NameSet(names = country_names, order=2)\n",
    "country_synth = [country_NS.make_name(add_to_history=False, n_candidates=5) for _ in range(250)]\n",
    "country_synth = list(set(country_synth))\n",
    "# Validate\n",
    "countries_validated = []\n",
    "for item in country_synth:\n",
    "    if validate_country_name(item):\n",
    "        pass\n",
    "    else:\n",
    "        if random.random() > 0.75:\n",
    "            template = random.choice(['the {name} Islands', 'the Republic of {name}', 'the {name} Kingdom', 'West {name}', 'East {name}', 'North {name}', 'South {name}', '{name}land'])\n",
    "            item = template.format(name=item)\n",
    "        countries_validated.append(item)\n",
    "        \n",
    "with open(f\"{parent_dir}/datasets/generators/synthetic/countries_raw.txt\", 'w') as f:\n",
    "    f.write(\"\\n\".join(map(str, countries_validated)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset of Unverifiable Statements\n",
    "Here, we load the list of names that we manually checked (i.e., filtered raw files)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synth_cities = pd.read_csv(f\"{parent_dir}/datasets/generators/synthetic/cities_checked.csv\")\n",
    "synth_cities = synth_cities[synth_cities['Keep'] == 1]\n",
    "synth_cities = synth_cities['Name'].tolist()\n",
    "\n",
    "synth_countries = pd.read_csv(f\"{parent_dir}/datasets/generators/synthetic/countries_checked.csv\")\n",
    "synth_countries = synth_countries[synth_countries['Keep'] == 1]\n",
    "synth_countries = synth_countries['Name'].tolist()\n",
    "random.seed(seed)\n",
    "synth_dict = {}\n",
    "for item in synth_cities:\n",
    "    synth_dict[item] = random.sample(synth_countries, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synth_db = CityCountry(synth_dict, category='cities', is_fake=True) #is_fake -> is_unverifiable\n",
    "synth_data = synth_db.generate_full_dataset()\n",
    "synth_data.write_json(f\"{parent_dir}/datasets/source/city_country_synthetic.json\")\n",
    "synth_subsample = synth_db.generate_subsample(seed=42, n=2000).with_columns(\n",
    "                 pl.col(\"correct_object_2\").list.join(\", \").alias(\"correct_object_2\"))\n",
    "synth_subsample.write_csv(f\"{parent_dir}/datasets/city_locations_synthetic.csv\")\n",
    "synth_subsample.group_by(['correct', 'negated']).len()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "belief-representation-dS6b1P8F-python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

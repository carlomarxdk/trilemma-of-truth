{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Definitions (Dataset)\n",
    "Here, we provide a code to collect and process the dataset of:\n",
    "1.  *Word* and corresponding *Synonym*,\n",
    "2.  *Word* and corresponding *Type*,\n",
    "3.  *Word* and corresponding *Instance*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import inflect\n",
    "import spacy\n",
    "import numpy as np\n",
    "from utils import WordsInstances, WordsSynonyms, WordsTypes, is_plural\n",
    "# imports\n",
    "from pathlib import Path\n",
    "# Parent directory\n",
    "parent_dir = str(Path().resolve().parents[0])\n",
    "import polars as pl\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Extract data from the `WordsAPI`\n",
    "You can download `wordsapi_sample.json` from the [WordsAPI Portal](https://www.wordsapi.com/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "def check_if_pnoun(term, definition):\n",
    "    \"\"\"\n",
    "    Check if the term is a proper noun\n",
    "    \"\"\"\n",
    "    term_len = len(term)\n",
    "    doc = nlp(f\"{term} is {definition}\")\n",
    "    for token in doc[:term_len]:\n",
    "        if token.pos_ == \"PROPN\":\n",
    "            ent_type = token.ent_type_\n",
    "            if ent_type == \"\":\n",
    "                return \"PROPN\"\n",
    "            else:\n",
    "                return ent_type\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check\n",
    "doc = nlp(\"Harrison is defined as a 9th president of the united states\")\n",
    "check_if_pnoun(\"madam\", \"a 9th president of the united states.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = [\"hasTypes\", \"typeOf\",\n",
    "                \"partOf\", \"hasParts\",\n",
    "                \"instanceOf\", \"hasInstances\",\n",
    "                \"memberOf\", \"hasMembers\",\n",
    "                \"substanceOf\", \"hasSubstances\",\n",
    "                \"inCategory\", \"hasCategories\",\n",
    "                \"regionOf\", \"inRegion\"]\n",
    "def check_attributes(word: dict):\n",
    "    keys = list(word.keys())\n",
    "    if \"definitions\" in keys:\n",
    "        if type(word[\"definitions\"]) == list:\n",
    "            for w in word[\"definitions\"]:\n",
    "                sub_keys = w.keys()\n",
    "                if \"synonyms\" in sub_keys and \"definition\" in sub_keys and \"partOfSpeech\" in sub_keys and any(keyword in sub_keys for keyword in keywords):\n",
    "                    pass\n",
    "                else:\n",
    "                    return False\n",
    "        else:\n",
    "            sub_keys = word[\"definitions\"].keys()\n",
    "            if \"synonyms\" in sub_keys and \"definition\" in sub_keys and \"partOfSpeech\" in sub_keys and any(keyword in sub_keys for keyword in keywords):\n",
    "                pass\n",
    "            else:\n",
    "                return False\n",
    "    else:\n",
    "        return False\n",
    "    if \"frequency\" not in keys:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "# Initialize the inflect engine\n",
    "p = inflect.engine()\n",
    "exceptions = (\"any\", \"one\", \"once\", 'something', 'someone', 'somebody', 'anything', 'anyone', 'anybody')\n",
    "def format_definitions(definition, part_of_speech):\n",
    "    # Remove text within brackets\n",
    "    definition = re.sub(r'\\s*\\(.*?\\)\\s*', ' ', definition).strip()\n",
    "    \n",
    "    # Check the first character of the definition to decide on the article\n",
    "    definition = definition.strip()\n",
    "    # Use the first part of the definition\\\n",
    "    definition = definition.split(';')[0]\n",
    "\n",
    "    if part_of_speech == \"noun\":\n",
    "        lower_definition = definition.lower()\n",
    "        \n",
    "        # Check specific cases where no article should be used\n",
    "        if lower_definition.startswith(exceptions):\n",
    "            article = ''\n",
    "        else:\n",
    "            # Use inflect to determine the correct article\n",
    "            article = p.a(definition).split()[0]\n",
    "        \n",
    "        # Check if the definition already starts with an article or 'the'\n",
    "        if not lower_definition.startswith(('a ', 'an ', 'the ')) and article:\n",
    "            definition = f\"{article} {definition}\"\n",
    "    elif part_of_speech == \"verb\":\n",
    "        if not definition.startswith(\"to \"):\n",
    "            definition = f\"to {definition}\"\n",
    "\n",
    "    if definition.endswith('.'):\n",
    "        definition = definition[:-1]\n",
    "    \n",
    "    return definition.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(parent_dir + '/source/wordsapi_sample.json') as f:\n",
    "    data = json.load(f)\n",
    "keys = data.keys()\n",
    "valid_keys = [key for key in keys if check_attributes(data[key])]\n",
    "print(\"Number of words:\", len(valid_keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = []\n",
    "extended_keywords = keywords + [\"definition\", \"synonyms\", \"partOfSpeech\"]\n",
    "for key in valid_keys:\n",
    "    word = data[key]\n",
    "    freq = word[\"frequency\"]\n",
    "    if type(freq) == dict:\n",
    "        zipf = freq[\"zipf\"]\n",
    "        perMillion = freq[\"perMillion\"]\n",
    "        diversity = freq[\"diversity\"]\n",
    "    else:\n",
    "        zipf = freq\n",
    "        perMillion = None\n",
    "        diversity = None\n",
    "\n",
    "    if \"letters\" in word.keys():\n",
    "        letters = word[\"letters\"]\n",
    "    else:\n",
    "        letters = None\n",
    "    if \"sounds\" in word.keys():\n",
    "        sounds = word[\"sounds\"]\n",
    "    else:\n",
    "        sounds = None\n",
    "    if type(word[\"definitions\"]) == list:\n",
    "        for w in word[\"definitions\"]:\n",
    "            _word = {k:v for k,v in w.items() if k in extended_keywords}\n",
    "            _word[\"word\"] = key\n",
    "            _word[\"zipf\"] = zipf\n",
    "            _word[\"perMillion\"] = perMillion\n",
    "            _word[\"diversity\"] = diversity\n",
    "            _word[\"letters\"] = letters\n",
    "            _word[\"sounds\"] = sounds\n",
    "            _word[\"definition\"] = format_definitions(_word[\"definition\"], _word[\"partOfSpeech\"])\n",
    "            _word[\"num_definitions\"] = len(word[\"definitions\"])\n",
    "            word_list.append(_word)\n",
    "    else:\n",
    "        _word = {k:v for k,v in word[\"definitions\"].items() if k in extended_keywords}\n",
    "        _word[\"word\"] = key\n",
    "        _word[\"zipf\"] = zipf\n",
    "        _word[\"perMillion\"] = perMillion\n",
    "        _word[\"diversity\"] = diversity\n",
    "        _word[\"letters\"] = letters\n",
    "        _word[\"sounds\"] = sounds\n",
    "        _word[\"definition\"] = format_definitions(_word[\"definition\"], _word[\"partOfSpeech\"])\n",
    "        _word[\"num_definitions\"] = 1\n",
    "\n",
    "        word_list.append(_word)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pl.from_dicts(word_list) \n",
    "## Check if PNOUN\n",
    "df = df.with_columns(\n",
    "    pl.struct([\"word\", \"definition\"]).map_elements(lambda x: check_if_pnoun(x[\"word\"], x[\"definition\"]), return_dtype=pl.String).alias(\"pnoun\")\n",
    ")\n",
    "# Reorder columns\n",
    "col_order = [\"word\", \"definition\", \"partOfSpeech\", \"pnoun\", \"synonyms\"] + keywords + [\"zipf\", \"perMillion\", \"diversity\", \"letters\", \"sounds\", \"num_definitions\"]\n",
    "col_order = [col for col in col_order if col in df.columns]\n",
    "df = df.select(col_order)\n",
    "## do some cleaning  (set to null if the PROPN or if adjective or verb)\n",
    "df = df.with_columns(pl.when( (pl.col(\"partOfSpeech\") == \"adjective\") | (pl.col(\"partOfSpeech\") == \"verb\"))\\\n",
    "                .then(None) \\\n",
    "                .otherwise(pl.col(\"pnoun\")) \\\n",
    "                .alias(\"pnoun\")) \\\n",
    "    .with_columns(pl.when(pl.col(\"pnoun\") == \"PROPN\") \\\n",
    "                .then(None) \\\n",
    "                .otherwise(pl.col(\"pnoun\")) \\\n",
    "                .alias(\"pnoun\")) \n",
    "\n",
    "df.write_json(parent_dir + \"/generators/definitions.json\")\n",
    "df.sample(20)\n",
    "df.filter(pl.col(\"pnoun\").is_not_null())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate `true` and `false` statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = inflect.engine()\n",
    "\n",
    "def is_plural(word):\n",
    "    \"\"\"\n",
    "    Check if a word is plural\n",
    "    \"\"\"\n",
    "    word = word.split(' ')[0]\n",
    "    check = p.singular_noun(word)\n",
    "    if type(check) == str:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_upper = ['ORG', 'FAC', 'GPE', 'PRODUCT']\n",
    "words2filter = pl.Series(['terrorist', 'terrorist group'])\n",
    "df = (\n",
    "    pl.read_json(parent_dir + \"/generators/definitions.json\")\n",
    "    .filter(pl.col(\"partOfSpeech\") == \"noun\")\n",
    "    .with_columns(\n",
    "        pl.when(pl.col(\"pnoun\").is_in(to_upper) & (pl.col(\"word\").str.len_chars() < 4))\n",
    "          .then(pl.col(\"word\").str.to_uppercase())\n",
    "          .otherwise(pl.col(\"word\"))\n",
    "          .alias(\"word\")\n",
    "    )\n",
    ")\n",
    "df = df.filter(~pl.col('instanceOf').list.contains('terrorist group') & ~pl.col('instanceOf').list.contains('terrorist') \\\n",
    "          & ~pl.col('instanceOf').list.contains('weapon') & ~pl.col('instanceOf').list.contains('ammunition') \\\n",
    "          &  ~pl.col('instanceOf').list.contains('firearm') & ~pl.col('instanceOf').list.contains('toxin'))\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_frequency = dict(df.select(\"word\", \"zipf\").iter_rows())\n",
    "instanceOf = dict(df.filter(pl.col(\"instanceOf\").is_not_null()).select(['word', 'instanceOf']).iter_rows())\n",
    "typeOf = dict(df.filter(pl.col(\"typeOf\").is_not_null()).select(['word', 'typeOf']).iter_rows())\n",
    "synonyms = dict(df.filter(pl.col(\"synonyms\").is_not_null() & pl.col('pnoun').is_null()).select(['word', 'synonyms']).iter_rows())\n",
    "len(instanceOf), len(typeOf), len(synonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "db_inst = WordsInstances(instanceOf, category='instances')\n",
    "data_inst = db_inst.generate_full_dataset()\n",
    "data_inst.write_json(f\"{parent_dir}/generators/word_instances.json\")\n",
    "db_type = WordsTypes(typeOf, category='types')\n",
    "data_type = db_type.generate_full_dataset()\n",
    "data_type.write_json(f\"{parent_dir}/generators/word_types.json\")\n",
    "db_synonym = WordsSynonyms(synonyms, category='synonyms')\n",
    "data_synonym = db_synonym.generate_full_dataset()\n",
    "data_synonym.write_json(f\"{parent_dir}/generators/word_synonyms.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordfreq import zipf_frequency\n",
    "def get_zipf(word):\n",
    "    return zipf_frequency(word, 'en')\n",
    "\n",
    "objects = list(set(db_inst.keys + db_type.keys + db_synonym.keys))\n",
    "objects_validated = [word for word in objects if get_zipf(word) > 0]\n",
    "len(objects), len(objects_validated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objects = set(db_inst.keys + db_type.keys + db_synonym.keys)\n",
    "objects = np.random.choice(list(objects), 800, replace=False)\n",
    "subsample_inst = db_inst.generate_subsample(1000, 42, objects).with_columns(\n",
    "                 pl.col(\"correct_object_2\").list.join(\", \").alias(\"correct_object_2\"))\n",
    "subsample_inst.write_csv(f\"{parent_dir}/word_instances.csv\")\n",
    "subsample_types = db_type.generate_subsample(2000, 42, objects=objects).with_columns(\n",
    "                 pl.col(\"correct_object_2\").list.join(\", \").alias(\"correct_object_2\"))\n",
    "subsample_types.write_csv(f\"{parent_dir}/word_types.csv\")\n",
    "\n",
    "subsample_synonyms = db_synonym.generate_subsample(2000, 42, objects=objects).with_columns(\n",
    "                    pl.col(\"correct_object_2\").list.join(\", \").alias(\"correct_object_2\"))\n",
    "subsample_synonyms.write_csv(f\"{parent_dir}/word_synonyms.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate Syntetic Entities\n",
    "Here, we generate synthetic names for countries and cities. \n",
    "Generated names are stored in `datasets/generators/synthetic/*_raw.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from english_words import get_english_words_set\n",
    "web_words = get_english_words_set(['web2'], lower=True)\n",
    "gcide_words = get_english_words_set(['gcide'], lower=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('words')\n",
    "from nltk.corpus import words\n",
    "english_words = set(words.words())\n",
    "\n",
    "def check_if_exists(word):\n",
    "    if word in english_words:\n",
    "        return True\n",
    "    if word.lower() in web_words:\n",
    "        return True\n",
    "    if word.lower() in gcide_words:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def check_if_full_exists(phrase):\n",
    "    return all([check_if_exists(word) for word in phrase.split()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_if_exists(\"owenster\"), check_if_full_exists('boaok cover'), check_if_full_exists('book cover')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from namemaker import NameSet\n",
    "import namemaker\n",
    "\n",
    "seed = 'udaxihhexdvxrcsnbacghqtargwuwr'\n",
    "random.seed(seed)\n",
    "namemaker_rng = namemaker.get_rng()\n",
    "namemaker_rng.seed(seed)\n",
    "\n",
    "our_vocab = sorted(df['word'].unique().to_list())\n",
    "word_NS = NameSet(names = our_vocab)\n",
    "word_synth = [word_NS.make_name(add_to_history=False) for _ in range(1000)]\n",
    "word_synth = list(set(word_synth))\n",
    "# Validate\n",
    "word_validated = []\n",
    "for item in word_synth:\n",
    "    if not check_if_exists(item):\n",
    "        word_validated.append(item)\n",
    "    else:\n",
    "        pass\n",
    "with open(f\"{parent_dir}/generators/synthetic/words_raw.txt\", 'w') as f:\n",
    "    f.write(\"\\n\".join(map(str, word_validated)))\n",
    "\n",
    "inst_vocab = set(db_inst.values).union(set(our_vocab))\n",
    "inst_NS = NameSet(names = inst_vocab)\n",
    "inst_synth = [inst_NS.make_name(add_to_history=False) for _ in range(1000)]\n",
    "inst_synth = list(set(inst_synth))\n",
    "inst_validated = []\n",
    "for item in inst_synth:\n",
    "    if not check_if_full_exists(item):\n",
    "        inst_validated.append(item)\n",
    "    else:\n",
    "        pass\n",
    "with open(f\"{parent_dir}/generators/synthetic/instances_raw.txt\", 'w') as f:\n",
    "    f.write(\"\\n\".join(map(str, inst_validated)))\n",
    "\n",
    "type_vocab = set(db_type.values).union(set(our_vocab))\n",
    "type_NS = NameSet(names = type_vocab)\n",
    "type_synth = [type_NS.make_name(add_to_history=False) for _ in range(1000)]\n",
    "type_synth = list(set(type_synth))\n",
    "type_validated = []\n",
    "for item in type_synth:\n",
    "    if not check_if_full_exists(item):\n",
    "        type_validated.append(item)\n",
    "    else:\n",
    "        pass\n",
    "with open(f\"{parent_dir}/generators/synthetic/types_raw.txt\", 'w') as f:\n",
    "    f.write(\"\\n\".join(map(str, type_validated)))\n",
    "\n",
    "\n",
    "synonyms_vocab = set(db_synonym.values).union(set(our_vocab))\n",
    "synonyms_NS = NameSet(names = synonyms_vocab)\n",
    "synonyms_synth = [synonyms_NS.make_name(add_to_history=False) for _ in range(1000)]\n",
    "synonyms_synth = list(set(synonyms_synth))\n",
    "synonyms_validated = []\n",
    "for item in synonyms_synth:\n",
    "    if not check_if_full_exists(item):\n",
    "        synonyms_validated.append(item)\n",
    "    else:\n",
    "        pass\n",
    "with open(f\"{parent_dir}/generators/synthetic/synonyms_raw.txt\", 'w') as f:\n",
    "    f.write(\"\\n\".join(map(str, synonyms_validated)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Create `unverifiable` statements\n",
    "Here, we load the list of names that we manually checked (i.e., filtered raw files)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synth_word2inst = {}\n",
    "for item in word_validated:\n",
    "    synth_word2inst[item] = random.sample(inst_validated, 2)\n",
    "db_syn_word2inst = WordsInstances(synth_word2inst, category='instances', is_fake=True)\n",
    "data_syn_word2inst = db_syn_word2inst.generate_full_dataset()\n",
    "data_syn_word2inst.write_json(f\"{parent_dir}/generators/synthetic/word2inst.json\")\n",
    "\n",
    "data_syn_word2inst = db_syn_word2inst.generate_subsample(500, 42).with_columns(\n",
    "                    pl.col(\"correct_object_2\").list.join(\", \").alias(\"correct_object_2\"))\n",
    "data_syn_word2inst.write_csv(f\"{parent_dir}/word_instances_synthetic.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synth_word2type = {}\n",
    "for item in word_validated:\n",
    "    synth_word2type[item] = random.sample(type_validated, 2)\n",
    "db_syn_word2type = WordsTypes(synth_word2type, category='types', is_fake=True)\n",
    "data_syn_word2type = db_syn_word2type.generate_full_dataset()\n",
    "data_syn_word2type.write_json(f\"{parent_dir}/generators/synthetic/word2type.json\")\n",
    "\n",
    "data_syn_word2type = db_syn_word2type.generate_subsample(1500, 42).with_columns(\n",
    "                    pl.col(\"correct_object_2\").list.join(\", \").alias(\"correct_object_2\"))\n",
    "data_syn_word2type.write_csv(f\"{parent_dir}/word_types_synthetic.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synth_word2syn = {}\n",
    "for item in word_validated:\n",
    "    synth_word2syn[item] = random.sample(synonyms_validated, 2)\n",
    "db_syn_word2syn = WordsSynonyms(synth_word2syn, category='synonyms', is_fake=True)\n",
    "data_syn_word2syn = db_syn_word2syn.generate_full_dataset()\n",
    "data_syn_word2syn.write_json(f\"{parent_dir}/generators/synthetic/word2syn.json\")\n",
    "\n",
    "data_syn_word2syn = db_syn_word2syn.generate_subsample(1500, 42).with_columns(\n",
    "                    pl.col(\"correct_object_2\").list.join(\", \").alias(\"correct_object_2\"))\n",
    "data_syn_word2syn.write_csv(f\"{parent_dir}/word_synonyms_synthetic.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "belief_representation-TQ_PkdhR-python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
